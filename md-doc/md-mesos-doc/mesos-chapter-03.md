#ç¬¬ä¸‰ç« . Libprocessåˆ†æ

libprocessæ˜¯mesosçš„RPCæ¡†æ¶ï¼Œå®ç°äº†actoré€šä¿¡æ¨¡å¼ã€‚

ä¸»è¦åŠŸèƒ½ç‚¹æ˜¯ï¼š

1. ç½‘ç»œIOï¼Œä½¿ç”¨çš„æ˜¯libevå¼‚æ­¥æ¡†æ¶ 
2. æ¶ˆæ¯ä¼ é€’å’Œè§£æï¼Œä¸»è¦æ˜¯Httpåè®®
3. äº‹ä»¶çš„æ´¾å‘å’Œå¤„ç†ã€‚å†…å­˜æ¶ˆæ¯é˜Ÿåˆ—

å·¥å…·ç±»ï¼š
é¦–å…ˆéœ€è¦åˆ†æçš„æ˜¯Futureå’ŒPromiseç±»ï¼Œè¿™ä¸¤ä¸ªç±»æ˜¯æœ€ä¸ºé‡è¦çš„åŸºç¡€ç±»ã€‚

**Future**      < include/process/future.hpp >


 ~~~cpp
 // Definition of a "shared" future. A future can hold any
// copy-constructible value. A future is considered "shared" because
// by default a future can be accessed concurrently.
template <typename T>
class Future
{
public:
  // Constructs a failed future.
  static Future<T> failed(const std::string& message);

  
  ...æ„é€ å‡½æ•°åˆ—è¡¨ï¼Œç•¥è¿‡
  
  // Helpers to get the current state of this future.
  bool isPending() const;
  bool isReady() const;
  bool isDiscarded() const;
  bool isFailed() const;
  bool hasDiscard() const;


  // Waits for this future to become ready, discarded, or failed.
  // é»˜è®¤æ— é™ç­‰å¾…
  bool await(const Duration& duration = Seconds(-1)) const;

  // Return the value associated with this future, waits indefinitely
  // until a value gets associated or until the future is discarded.
  const T& get() const;
  const T* operator->() const;
  
  // å›è°ƒç›¸å…³
  // Type of the callback functions that can get invoked when the
  // future gets set, fails, or is discarded.
  typedef lambda::function<void()> DiscardCallback;
  typedef lambda::function<void(const T&)> ReadyCallback;
  typedef lambda::function<void(const std::string&)> FailedCallback;
  typedef lambda::function<void()> DiscardedCallback;
  typedef lambda::function<void(const Future<T>&)> AnyCallback;

  // Installs callbacks for the specified events and returns a const
  // reference to 'this' in order to easily support chaining.
  const Future<T>& onDiscard(DiscardCallback&& callback) const;
  const Future<T>& onReady(ReadyCallback&& callback) const;
  const Future<T>& onFailed(FailedCallback&& callback) const;
  const Future<T>& onDiscarded(DiscardedCallback&& callback) const;
  const Future<T>& onAny(AnyCallback&& callback) const;

 ......
 
 private:
  friend class Promise<T>;
  friend class WeakFuture<T>;

  enum State
  {
    PENDING,
    READY,
    FAILED,
    DISCARDED,
  };
  
 ~~~

ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼ŒFutureä¸€å…±æœ‰å››ç§çŠ¶æ€ï¼ŒæŒ‚èµ·ï¼Œå°±ç»ªï¼Œå¤±è´¥å’Œä¸¢å¼ƒã€‚

 ~~~cpp

  struct Data
  {
    Data();
    ~Data() = default;

    void clearAllCallbacks();

    std::atomic_flag lock = ATOMIC_FLAG_INIT;
    State state;
    bool discard;
    bool associated;



 ~~~cpp

    // One of:
    //   1. None, the state is PENDING or DISCARDED.
    //   2. Some, the state is READY.
    //   3. Error, the state is FAILED; 'error()' stores the message.
    Result<T> result;

    std::vector<DiscardCallback> onDiscardCallbacks;
    std::vector<ReadyCallback> onReadyCallbacks;
    std::vector<FailedCallback> onFailedCallbacks;
    std::vector<DiscardedCallback> onDiscardedCallbacks;
    std::vector<AnyCallback> onAnyCallbacks;
  };

  // Sets the value for this future, unless the future is already set,
  // failed, or discarded, in which case it returns false.
  bool set(const T& _t);
  
  //è¿™é‡Œæ˜¯ç›¸å…³çš„Promiseå¯¹è±¡è®¾ç½®çš„ï¼Œæ‰€ä»¥ä¸æ˜¯publicçš„ï¼ŒPromiseå’ŒFutureéƒ½æ˜¯æˆåŒæˆå¯¹å‡ºç°çš„ã€‚

  // Sets this future as failed, unless the future is already set,
  // failed, or discarded, in which case it returns false.
  bool fail(const std::string& _message);

 //è¿™é‡Œä¹Ÿæ˜¯ç›¸å…³çš„Promiseå¯¹è±¡è®¾ç½®çš„ï¼Œæ‰€ä»¥ä¸æ˜¯publicçš„ï¼ŒPromiseå’ŒFutureéƒ½æ˜¯æˆåŒæˆå¯¹å‡ºç°çš„ã€‚

  std::shared_ptr<Data> data;
};
 ~~~

ä¸Šé¢é¦–å…ˆåˆ—å‡ºFutureçš„æ•°æ®æˆå‘˜ï¼Œåç»­ä½¿ç”¨åˆ°çš„è¯ä¼šé€æ­¥å¼•å‡ºå®ƒçš„æˆå‘˜å‡½æ•°ã€‚

ç”±ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œä¸€ä¸ªFutureåŒ…å«å››ä¸ªçŠ¶æ€ï¼Œæš‚åœï¼Œå°±ç»ªï¼Œå¤±è´¥ï¼Œä¸¢å¼ƒã€‚å„ç§çŠ¶æ€æœ‰ä¸åŒçš„å›è°ƒå‡½æ•°å¯ä»¥ç”¨æ¥å¤„ç†ã€‚     

ä¸‹é¢ç»“åˆPromiseçœ‹çœ‹ä¸¤è€…çš„å®ç°é€»è¾‘ï¼š

 ~~~cpp
template <typename T>
class Promise
{
public:
  Promise();
  explicit Promise(const T& t);
  virtual ~Promise();

  Promise(Promise<T>&& that);

  bool discard();
  bool set(const T& _t);
  bool set(const Future<T>& future); // Alias for associate.
  bool associate(const Future<T>& future);
  bool fail(const std::string& message);

  // Returns a copy of the future associated with this promise.
  Future<T> future() const;

private:
  template <typename U>
  friend void internal::discarded(Future<U> future);

  // Not copyable, not assignable.
  Promise(const Promise<T>&);
  Promise<T>& operator=(const Promise<T>&);

  // Helper for doing the work of actually discarding a future (called
  // from Promise::discard as well as internal::discarded).
  static bool discard(Future<T> future);

  Future<T> f;
};
 ~~~
 
  ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼ŒPromiseåŒ…å«äº†ä¸€ä¸ª
  Futureï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦å®ç°Promise-Futureé€»è¾‘ï¼Œä¸€å®šè¦ä½¿ç”¨
  Promiseçš„future()å‡½æ•°è¿”å›çš„Futureï¼Œæ‰èƒ½å°†ä¸¤è€…å…³è”èµ·æ¥ã€‚
  å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡associateæ‰‹å·¥å…³è”ä¸¤ä¸ªå¯¹è±¡ã€‚


å¦‚æœåœ¨æ„é€ å‡½æ•°é‡Œé¢ç›´æ¥ç»™Promiseè®¾ç½®ä¸€ä¸ªå€¼ï¼Œ

 ~~~cpp
template < typename T >
Promise<T>::Promise(const T& t)
      : f(t) {}  
 ~~~  
åˆ™ä¼šç›´æ¥è§¦å‘Futureçš„æ„é€ å‡½æ•°ï¼š

~~~cpp

template <typename T>
Future<T>::Future(const T& _t)
  : data(new Data())
{
  set(_t);
}
~~~
~~~cpp
template <typename T>
bool Future<T>::set(const T& _t)
{
  bool result = false;

  synchronized (data->lock) {
    if (data->state == PENDING) {
      data->result = _t;
      data->state = READY;
      result = true;
    }
  }

  // Invoke all callbacks associated with this future being READY. We
  // don't need a lock because the state is now in READY so there
  // should not be any concurrent modications.
  if (result) {
    internal::run(data->onReadyCallbacks, data->result.get());
    internal::run(data->onAnyCallbacks, *this);

    data->clearAllCallbacks();
  }

  return result;
}

~~~

ç„¶åæˆ‘ä»¬åˆ†æä¸€ä¸‹Futureçš„getå‡½æ•°ï¼š

~~~cpp
template <typename T>
const T& Future<T>::get() const
{
  if (!isReady()) {
    await();
  }

  CHECK(!isPending()) << "Future was in PENDING after await()";
  // We can't use CHECK_READY here due to check.hpp depending on future.hpp.
  if (!isReady()) {
    CHECK(!isFailed()) << "Future::get() but state == FAILED: " << failure();
    CHECK(!isDiscarded()) << "Future::get() but state == DISCARDED";
  }

  assert(data->result.isSome());
  return data->result.get();
}
~~~

ä»ä¸Šé¢å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœæ•°æ®æ²¡æœ‰å‡†å¤‡å¥½ï¼Œgetæ˜¯ä¼šé˜»å¡çš„ï¼Œä¸€ç›´é˜»å¡åœ¨awaitå‡½æ•°é‡Œé¢ã€‚

 ~~~cpp
namespace internal {

inline void awaited(Owned<Latch> latch)
{
  latch->trigger();
}

} // namespace internal {


template <typename T>
bool Future<T>::await(const Duration& duration) const
{
  ...
  
  Owned<Latch> latch(new Latch());

  bool pending = false;

  synchronized (data->lock) {
    if (data->state == PENDING) {
      pending = true;
      data->onAnyCallbacks.push_back(lambda::bind(&internal::awaited, latch));
    }
  }

  if (pending) {
    return latch->await(duration);
  }

  return true;
}
 ~~~
    
 å½“çŠ¶æ€ä¸ºPENDINGçš„æ—¶å€™ï¼Œå°†æ­¤Futureçš„ä»»ä½•çŠ¶æ€å˜åŒ–å‡é€šè¿‡internal::awaitedå›è°ƒæ¥è§¦å‘latchï¼Œå¦‚æœLatchå·²ç»è¢«è§¦å‘äº†ï¼Œåˆ™ç›´æ¥è¿”å›trueã€‚
 
 *src/latch.cpp*
 
 ~~~cpp
 
bool Latch::trigger()
{
  bool expected = false;
  if (triggered.compare_exchange_strong(expected, true)) {
    terminate(pid);
    return true;
  }
  return false;
}


bool Latch::await(const Duration& duration)
{
  if (!triggered.load()) {
    process::wait(pid, duration); 
    ...
    return triggered.load();
  }

  return true;
}
 ~~~

**EventåŸºç±»**


~~~cpp
struct EventVisitor
{
  virtual ~EventVisitor() {}
  virtual void visit(const MessageEvent& event) {}
  virtual void visit(const DispatchEvent& event) {}
  virtual void visit(const HttpEvent& event) {}
  virtual void visit(const ExitedEvent& event) {}
  virtual void visit(const TerminateEvent& event) {}
};


struct Event
{
  virtual ~Event() {}

  virtual void visit(EventVisitor* visitor) const = 0;

  template <typename T>
  bool is() const
  {
    bool result = false;
    struct IsVisitor : EventVisitor
    {
      explicit IsVisitor(bool* _result) : result(_result) {}
      virtual void visit(const T& t) { *result = true; }
      bool* result;
    } visitor(&result);
    visit(&visitor);
    return result;
  }

  template <typename T>
  const T& as() const
  {
    const T* result = NULL;
    struct AsVisitor : EventVisitor
    {
      explicit AsVisitor(const T** _result) : result(_result) {}
      virtual void visit(const T& t) { *result = &t; }
      const T** result;
    } visitor(&result);
    visit(&visitor);
    if (result == NULL) {
      ABORT("Attempting to \"cast\" event incorrectly!");
    }
    return *result;
  }
};

~~~	
	
è¿™ä¸¤ä¸ªç±»å¯ä»¥å¥½å¥½æ¶ˆåŒ–ä¸€ä¸‹ï¼Œéœ€è¦å¯¹è™šå‡½æ•°çš„æ‰§è¡Œæœºåˆ¶æœ‰éå¸¸æ¸…æ¥šçš„äº†è§£ï¼Œæ–¹ä¾¿çš„å®ç°äº†dynamic_castçš„æœºåˆ¶ã€‚

~~~cpp
struct ExitedEvent : Event
{
  explicit ExitedEvent(const UPID& _pid)
    : pid(_pid) {}

  virtual void visit(EventVisitor* visitor) const
  {
    visitor->visit(*this);
  }

  const UPID pid;

private:
  // Keep ExitedEvent not assignable even though we made it copyable.
  // Note that we are violating the "rule of three" here but it helps
  // keep the fields const.
  ExitedEvent& operator=(const ExitedEvent&);
};
~~~

**TODOï¼š**  è¡¥å……ä¸€äº›ä¾‹å­


æœ€ä¸»è¦çš„å‡½æ•°å…¥å£å°±æ˜¯ include/process/process.hpp é‡Œé¢çš„è¿™ä¸ªå‡½æ•°ï¼š

 ~~~cpp
void initialize(const std::string& delegate = "");

void initialize(const string& delegate)
{
  // Create a new ProcessManager and SocketManager.
  process_manager = new ProcessManager(delegate);
  socket_manager = new SocketManager();

  // Initialize the event loop.
  EventLoop::initialize();   //1ï¸âƒ£ 
  
  // Setup processing threads.
  long cpus = process_manager->init_threads(); //2ï¸âƒ£

  Clock::initialize(lambda::bind(&timedout, lambda::_1));

  __address__ = Address::LOCALHOST_ANY();

  // Check environment for ip.
  Option<string> value = os::getenv("LIBPROCESS_IP");
  if (value.isSome()) {
    Try<net::IP> ip = net::IP::parse(value.get(), AF_INET);
    if (ip.isError()) {
      LOG(FATAL) << "Parsing LIBPROCESS_IP=" << value.get()
                 << " failed: " << ip.error();
    }
    __address__.ip = ip.get();
  }

  // Check environment for port.
  value = os::getenv("LIBPROCESS_PORT");
  if (value.isSome()) {
    Try<int> result = numify<int>(value.get().c_str());
    if (result.isSome() && result.get() >=0 && result.get() <= USHRT_MAX) {
      __address__.port = result.get();
    } else {
      LOG(FATAL) << "LIBPROCESS_PORT=" << value.get() << " is not a valid port";
    }
  }

  // Create a "server" socket for communicating.
  Try<Socket> create = Socket::create();
  if (create.isError()) {
    PLOG(FATAL) << "Failed to construct server socket:" << create.error();
  }
  __s__ = new Socket(create.get());

  // Allow address reuse.
  int on = 1;
  if (setsockopt(__s__->get(), SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on)) < 0) {
    PLOG(FATAL) << "Failed to initialize, setsockopt(SO_REUSEADDR)";
  }

  Try<Address> bind = __s__->bind(__address__);
  if (bind.isError()) {
    PLOG(FATAL) << "Failed to initialize: " << bind.error();
  }

  __address__ = bind.get();

  // If advertised IP and port are present, use them instead.
  value = os::getenv("LIBPROCESS_ADVERTISE_IP");
  if (value.isSome()) {
    Try<net::IP> ip = net::IP::parse(value.get(), AF_INET);
    if (ip.isError()) {
      LOG(FATAL) << "Parsing LIBPROCESS_ADVERTISE_IP=" << value.get()
                 << " failed: " << ip.error();
    }
    __address__.ip = ip.get();
  }

  value = os::getenv("LIBPROCESS_ADVERTISE_PORT");
  if (value.isSome()) {
    Try<int> result = numify<int>(value.get().c_str());
    if (result.isSome() && result.get() >=0 && result.get() <= USHRT_MAX) {
      __address__.port = result.get();
    } else {
      LOG(FATAL) << "LIBPROCESS_ADVERTISE_PORT=" << value.get()
                 << " is not a valid port";
    }
  }

  // Lookup hostname if missing ip or if ip is 0.0.0.0 in case we
  // actually have a valid external ip address. Note that we need only
  // one ip address, so that other processes can send and receive and
  // don't get confused as to whom they are sending to.
  if (__address__.ip.isAny()) {
    char hostname[512];

    if (gethostname(hostname, sizeof(hostname)) < 0) {
      LOG(FATAL) << "Failed to initialize, gethostname: "
                 << hstrerror(h_errno);
    }

    // Lookup IP address of local hostname.
    Try<net::IP> ip = net::getIP(hostname, __address__.ip.family());

    if (ip.isError()) {
      EXIT(EXIT_FAILURE)
        << "Failed to obtain the IP address for '" << hostname << "';"
        << " the DNS service may not be able to resolve it: " << ip.error();
    }

    __address__.ip = ip.get();
  }

  Try<Nothing> listen = __s__->listen(LISTEN_BACKLOG);
  if (listen.isError()) {
    PLOG(FATAL) << "Failed to initialize: " << listen.error();
  }

  // Need to set `initialize_complete` here so that we can actually
  // invoke `accept()` and `spawn()` below.
  initialize_complete.store(true);

  __s__->accept()
    .onAny(lambda::bind(&internal::on_accept, lambda::_1));//3ï¸âƒ£

  // TODO(benh): Make sure creating the garbage collector, logging
  // process, and profiler always succeeds and use supervisors to make
  // sure that none terminate.

  // Create global garbage collector process.
  gc = spawn(new GarbageCollector());//4ï¸âƒ£

  // Create global help process.
  help = spawn(new Help(), true);//5ï¸âƒ£

  // Create the global logging process.
  spawn(new Logging(), true);//6ï¸âƒ£

  // Create the global profiler process.
  spawn(new Profiler(), true);//7ï¸âƒ£

  // Create the global system statistics process.
  spawn(new System(), true);//8ï¸âƒ£

  // Create the global HTTP authentication router.
  authenticator_manager = new AuthenticatorManager();//9ï¸âƒ£

  // Ensure metrics process is running.
  // TODO(bmahler): Consider initializing this consistently with
  // the other global Processes.
  metrics::internal::MetricsProcess* metricsProcess =
    metrics::internal::MetricsProcess::instance();

  CHECK_NOTNULL(metricsProcess);

  // Initialize the mime types.
  mime::initialize();

  // Add a route for getting process information.
  lambda::function<Future<Response>(const Request&)> __processes__ =
    lambda::bind(&ProcessManager::__processes__, process_manager, lambda::_1);

  new Route("/__processes__", None(), __processes__);//ğŸ”Ÿ

  VLOG(1) << "libprocess is initialized on " << address() << " for " << cpus
          << " cpus";
}

 ~~~

**å…¨å±€å˜é‡**

ä¸‹é¢æ˜¯å®ç°çš„cppæ–‡ä»¶é‡Œé¢çš„å‡ ä¸ªé™æ€å…¨å±€å˜é‡ï¼Œä¸»è¦çš„é€»è¾‘åœ¨è¿™é‡Œï¼š

 ~~~cpp
 // Server socket listen backlog.
static const int LISTEN_BACKLOG = 500000;

// Local server socket.
static Socket* __s__ = NULL;

// Local socket address.
static Address __address__;

// Active SocketManager (eventually will probably be thread-local).
static SocketManager* socket_manager = NULL;

// Active ProcessManager (eventually will probably be thread-local).
static ProcessManager* process_manager = NULL;

// Scheduling gate that threads wait at when there is nothing to run.
static Gate* gate = new Gate();

// Used for authenticating HTTP requests.
static AuthenticatorManager* authenticator_manager = NULL;
// Filter. Synchronized support for using the filterer needs to be
// recursive in case a filterer wants to do anything fancy (which is
// possible and likely given that filters will get used for testing).
static Filter* filterer = NULL;
static std::recursive_mutex* filterer_mutex = new std::recursive_mutex();

// Global garbage collector.
PID<GarbageCollector> gc;

// Global help.
PID<Help> help;

// Per thread process pointer.
THREAD_LOCAL ProcessBase* __process__ = NULL;

// Per thread executor pointer.
THREAD_LOCAL Executor* _executor_ = NULL;
 ~~~
**å·¥å…·ç±»**


 
 

1ï¸âƒ£ç½‘ç»œçš„åˆå§‹åŒ–

 ~~~cpp
 //1ï¸âƒ£
event_base* base = NULL;

void EventLoop::initialize()
{
  static Once* initialized = new Once();

  if (initialized->once()) {
    return;
  }

  ...
  
  struct event_config* config = event_config_new();
  event_config_avoid_method(config, "epoll");

  base = event_base_new_with_config(config);

  initialized->done();
}

æˆ–è€… libev.cpp

ev_async async_watcher;
// We need an asynchronous watcher to receive the request to shutdown.
ev_async shutdown_watcher;

// Define the initial values for all of the declarations made in
// libev.hpp (since these need to live in the static data space).
struct ev_loop* loop = NULL;


THREAD_LOCAL bool* _in_event_loop_ = NULL;

void EventLoop::initialize()
{
  loop = ev_default_loop(EVFLAG_AUTO);

  ev_async_init(&async_watcher, handle_async);
  ev_async_init(&shutdown_watcher, handle_shutdown);

  ev_async_start(loop, &async_watcher);
  ev_async_start(loop, &shutdown_watcher);
}

void handle_async(struct ev_loop* loop, ev_async* _, int revents)
{
  std::queue<lambda::function<void()>> run_functions;
  synchronized (watchers_mutex) {
    // Start all the new I/O watchers.
    while (!watchers->empty()) {
      ev_io* watcher = watchers->front();
      watchers->pop();
      ev_io_start(loop, watcher);
    }

    // Swap the functions into a temporary queue so that we can invoke
    // them outside of the mutex.
    std::swap(run_functions, *functions);
  }

  // Running the functions outside of the mutex reduces locking
  // contention as these are arbitrary functions that can take a long
  // time to execute. Doing this also avoids a deadlock scenario where
  // (A) mutexes are acquired before calling `run_in_event_loop`,
  // followed by locking (B) `watchers_mutex`. If we executed the
  // functions inside the mutex, then the locking order violation
  // would be this function acquiring the (B) `watchers_mutex`
  // followed by the arbitrary function acquiring the (A) mutexes.
  while (!run_functions.empty()) {
    (run_functions.front())();
    run_functions.pop();
  }
}

 ~~~
 
 2ï¸âƒ£åˆå§‹åŒ–çº¿ç¨‹
 
 ~~~cpp
class ProcessManager
{
   ...
    // Boolean used to signal processing threads to stop running.
  std::atomic_bool joining_threads;
}

long ProcessManager::init_threads()
{
  joining_threads.store(false);

  // We create no fewer than 8 threads because some tests require
  // more worker threads than `sysconf(_SC_NPROCESSORS_ONLN)` on
  // computers with fewer cores.
  // e.g. https://issues.apache.org/jira/browse/MESOS-818
  //
  // TODO(xujyan): Use a smarter algorithm to allocate threads.
  // Allocating a static number of threads can cause starvation if
  // there are more waiting Processes than the number of worker
  // threads.
  long cpus = std::max(8L, sysconf(_SC_NPROCESSORS_ONLN));
  threads.reserve(cpus+1);

  // Create processing threads.
  for (long i = 0; i < cpus; i++) {
    // Retain the thread handles so that we can join when shutting down.
    threads.emplace_back(
        // We pass a constant reference to `joining` to make it clear that this
        // value is only being tested (read), and not manipulated.
        new std::thread(std::bind([](const std::atomic_bool& joining) {
          do {
          
          //è¿™é‡Œæœ‰ç‚¹double checkçš„å‘³é“ã€‚
            ProcessBase* process = process_manager->dequeue();
            if (process == NULL) {
              //é˜Ÿåˆ—é‡Œé¢æ²¡æœ‰æ¶ˆæ¯éœ€è¦å¤„ç†ï¼ŒIdleè®¡æ•°å™¨åŠ 1
              Gate::state_t old = gate->approach();
              process = process_manager->dequeue();
              if (process == NULL) {
                if (joining.load()) {
                  break;
                }
                //ä¸€ç›´ç­‰å¾…é˜Ÿåˆ—æœ‰æ•°æ®ä¸ºæ­¢ã€‚çœ‹æ¥å¾€é˜Ÿåˆ—é‡Œé‡Œé¢æ’å…¥æ•°æ®æ—¶ä¸€å®šä¼šè°ƒç”¨Gateçš„æŸä¸ªæ–¹æ³•æ¥é€šçŸ¥è¿™é‡Œã€‚
                gate->arrive(old); // Wait at gate if idle.
                continue;
              } else {
                gate->leave();
              }
            }
            process_manager->resume(process);  //âœº11
          } while (true);
        },
        std::cref(joining_threads))));
  }

  // Create a thread for the event loop.
  threads.emplace_back(new std::thread(&EventLoop::run)); //âœº12

  return cpus;
}
 ~~~
 
 ä¸»è¦æ˜¯æ ¹æ®CPUä¸ªæ•°å¯åŠ¨äº†Nä¸ªçº¿ç¨‹ï¼Œä»¥åŠEventLoopçš„ä¸€ä¸ªçº¿ç¨‹ã€‚
 Workerçº¿ç¨‹ä¸»è¦æ˜¯ç›‘å¬ProcessManagerçš„æ¶ˆæ¯é˜Ÿåˆ—ï¼Œå¦‚æœæœ‰æ•°æ®ï¼Œåˆ™è¿›è¡Œå¤„ç†ï¼Œæ²¡æœ‰æ•°æ®åˆ™é˜»å¡ç­‰å¾…ã€‚
 

æˆ‘ä»¬é¦–å…ˆåˆ†æ âœº12 ï¼š

 ~~~cpp
class EventLoop
{
public:
  ...
  static void run();
};

extern THREAD_LOCAL bool* _in_event_loop_;

#define __in_event_loop__ *(_in_event_loop_ == NULL ?                \
  _in_event_loop_ = new bool(false) : _in_event_loop_)


void EventLoop::run()
{
  __in_event_loop__ = true;

  do {
    int result = event_base_loop(base, EVLOOP_ONCE);
    if (result < 0) {
      LOG(FATAL) << "Failed to run event loop";
    } else if (result > 0) {
      // All events are handled, continue event loop.
      continue;
    } else {
      CHECK_EQ(0, result);
      if (event_base_got_break(base)) {
        break;
      } else if (event_base_got_exit(base)) {
        break;
      }
    }
  } while (true);

  __in_event_loop__ = false;
}
 ~~~
è¿™é‡Œä»…ä»…æ˜¯å¯åŠ¨äº†ç½‘ç»œçš„äº‹ä»¶ç›‘å¬å¾ªç¯ã€‚

 ~~~cpp
class Socket
{
public:
 ...
  class Impl : public std::enable_shared_from_this<Impl>
  {
  public:
    virtual Try<Nothing> listen(int backlog) = 0;
    virtual Future<Socket> accept() = 0;
  protected:
    int s;
  };
  
  Future<Socket> accept()
  {
    return impl->accept();
  }
private:  
  std::shared_ptr<Impl> impl;
};

class PollSocketImpl : public Socket::Impl
{
public:
  ...
  virtual Future<Socket> accept();
  };
};

namespace internal {

Future<Socket> accept(int fd)
{
  Try<int> accepted = network::accept(fd);
  if (accepted.isError()) {
    return Failure(accepted.error());
  }

  int s = accepted.get();
  Try<Nothing> nonblock = os::nonblock(s);
  if (nonblock.isError()) {
    LOG_IF(INFO, VLOG_IS_ON(1)) << "Failed to accept, nonblock: "
                                << nonblock.error();
    os::close(s);
    return Failure("Failed to accept, nonblock: " + nonblock.error());
  }

  Try<Nothing> cloexec = os::cloexec(s);
  if (cloexec.isError()) {
    LOG_IF(INFO, VLOG_IS_ON(1)) << "Failed to accept, cloexec: "
                                << cloexec.error();
    os::close(s);
    return Failure("Failed to accept, cloexec: " + cloexec.error());
  }

  // Turn off Nagle (TCP_NODELAY) so pipelined requests don't wait.
  int on = 1;
  if (setsockopt(s, SOL_TCP, TCP_NODELAY, &on, sizeof(on)) < 0) {
    const string error = os::strerror(errno);
    VLOG(1) << "Failed to turn off the Nagle algorithm: " << error;
    os::close(s);
    return Failure(
      "Failed to turn off the Nagle algorithm: " + stringify(error));
  }

  Try<Socket> socket = Socket::create(Socket::DEFAULT_KIND(), s);
  if (socket.isError()) {
    os::close(s);
    return Failure("Failed to accept, create socket: " + socket.error());
  }
  return socket.get();
}

} // namespace internal {


Future<Socket> PollSocketImpl::accept()
{
  return io::poll(get(), io::READ)
    .then(lambda::bind(&internal::accept, get()));
}
 ~~~
 è¿™é‡Œå°±æ˜¯ä½¿ç”¨libevçš„äº‹ä»¶å¾ªç¯æœºåˆ¶æ¥å®ç°äº†
 
 ~~~cpp


Future<short> poll(int fd, short events)
{
  process::initialize();

  // TODO(benh): Check if the file descriptor is non-blocking?

  return run_in_event_loop<short>(lambda::bind(&internal::poll, fd, events));
}

// Helper for running a function in the event loop.
template <typename T>
Future<T> run_in_event_loop(const lambda::function<Future<T>()>& f)
{
  // If this is already the event loop then just run the function.
  if (__in_event_loop__) {
    return f();
  }

  Owned<Promise<T>> promise(new Promise<T>());

  Future<T> future = promise->future();

  // Enqueue the function.
  synchronized (watchers_mutex) {
    functions->push(lambda::bind(&_run_in_event_loop<T>, f, promise));
  }

  // Interrupt the loop.
  ev_async_send(loop, &async_watcher);

  return future;
}
 ~~~
libevé‡Œé¢ï¼Œasync_watcherçš„å›è°ƒå‡½æ•°æ˜¯ï¼š

 ~~~cpp
 std::queue<ev_io*>* watchers = new std::queue<ev_io*>();

std::mutex* watchers_mutex = new std::mutex();

std::queue<lambda::function<void()>>* functions =
  new std::queue<lambda::function<void()>>();

void handle_async(struct ev_loop* loop, ev_async* _, int revents)
{
  std::queue<lambda::function<void()>> run_functions;
  synchronized (watchers_mutex) {
    // Start all the new I/O watchers.
    while (!watchers->empty()) {
      ev_io* watcher = watchers->front();
      watchers->pop();
      ev_io_start(loop, watcher);
    }

    // Swap the functions into a temporary queue so that we can invoke
    // them outside of the mutex.
    std::swap(run_functions, *functions);
  }

  // Running the functions outside of the mutex reduces locking
  // contention as these are arbitrary functions that can take a long
  // time to execute. Doing this also avoids a deadlock scenario where
  // (A) mutexes are acquired before calling `run_in_event_loop`,
  // followed by locking (B) `watchers_mutex`. If we executed the
  // functions inside the mutex, then the locking order violation
  // would be this function acquiring the (B) `watchers_mutex`
  // followed by the arbitrary function acquiring the (A) mutexes.
  while (!run_functions.empty()) {
    (run_functions.front())();
    run_functions.pop();
  }
}

 ~~~
 
 ä¸Šè¿°å°±æ˜¯ä¸ºäº†ç¡®ä¿pollæ˜¯åœ¨æŸä¸ªçº¿ç¨‹çš„evloopé‡Œé¢å»æ‰§è¡Œï¼Œè¿™æ—¶å€™evloopå·²ç»åˆ›å»ºæˆåŠŸäº†ã€‚æœ€ç»ˆè°ƒç”¨çš„æ˜¯å‡½æ•°internal::pollï¼š
 
 ~~~cpp
 struct Poll
{
  Poll()
  {
    // Need to explicitly instantiate the watchers.
    watcher.io.reset(new ev_io());
    watcher.async.reset(new ev_async());
  }

  // An I/O watcher for checking for readability or writeability and
  // an async watcher for being able to discard the polling.
  struct {
    std::shared_ptr<ev_io> io;
    std::shared_ptr<ev_async> async;
  } watcher;

  Promise<short> promise;
};
 ~~~
 ä¸ºäº†æ–¹ä¾¿å›è°ƒå¤„ç†ï¼Œè®¾ç½®äº†ä¸Šè¿°ç±»å‹Poll
 
 ~~~cpp

// Event loop callback when I/O is ready on polling file descriptor.
void polled(struct ev_loop* loop, ev_io* watcher, int revents)
{
  Poll* poll = (Poll*) watcher->data;

  ev_io_stop(loop, poll->watcher.io.get());

  // Stop the async watcher (also clears if pending so 'discard_poll'
  // will not get invoked and we can delete 'poll' here).
  ev_async_stop(loop, poll->watcher.async.get());

  poll->promise.set(revents);

  delete poll;
}


// Event loop callback when future associated with polling file
// descriptor has been discarded.
void discard_poll(struct ev_loop* loop, ev_async* watcher, int revents)
{
  Poll* poll = (Poll*) watcher->data;

  // Check and see if we have a pending 'polled' callback and if so
  // let it "win".
  if (ev_is_pending(poll->watcher.io.get())) {
    return;
  }

  ev_async_stop(loop, poll->watcher.async.get());

  // Stop the I/O watcher (but note we check if pending above) so it
  // won't get invoked and we can delete 'poll' here.
  ev_io_stop(loop, poll->watcher.io.get());

  poll->promise.discard();

  delete poll;
}


namespace io {
namespace internal {

// Helper/continuation of 'poll' on future discard.
void _poll(const std::shared_ptr<ev_async>& async)
{
  ev_async_send(loop, async.get());
}


Future<short> poll(int fd, short events)
{
  Poll* poll = new Poll();

  // Have the watchers data point back to the struct.
  poll->watcher.async->data = poll;
  poll->watcher.io->data = poll;

  // Get a copy of the future to avoid any races with the event loop.
  Future<short> future = poll->promise.future();

  // Initialize and start the async watcher.
  ev_async_init(poll->watcher.async.get(), discard_poll);âœº20
  ev_async_start(loop, poll->watcher.async.get());

  // Make sure we stop polling if a discard occurs on our future.
  // Note that it's possible that we'll invoke '_poll' when someone
  // does a discard even after the polling has already completed, but
  // in this case while we will interrupt the event loop since the
  // async watcher has already been stopped we won't cause
  // 'discard_poll' to get invoked.
  
  //æ³¨æ„ï¼Œè¿™é‡Œè®¾ç½®äº†onDiscardå›è°ƒï¼Œå½“watcher.asyncçš„äº‹ä»¶è¢«å›è°ƒæ—¶ï¼Œä¼šè°ƒç”¨è¿™é‡Œè®¾ç½®çš„æ–¹æ³•ã€‚
  future.onDiscard(lambda::bind(&_poll, poll->watcher.async));âœº21

  // Initialize and start the I/O watcher.
  ev_io_init(poll->watcher.io.get(), polled, fd, events);âœº22
  ev_io_start(loop, poll->watcher.io.get());

  return future;
}

} 

 ~~~
 ä»âœº20å¤„å¯ä»¥çœ‹åˆ°ï¼Œä¸€å®šæ˜¯åœ¨æŸä¸ªåœ°æ–¹å¯ä»¥è§¦å‘åœæ­¢ç›‘å¬poolçš„ï¼Œä½†æ˜¯ä»ç°åœ¨çš„åˆ†æä¸­è¿˜æ²¡æœ‰çœ‹åˆ°ï¼Œæˆ‘ä»¬çœ‹çœ‹åç»­çš„ä»£ç åˆ†æä¸­èƒ½å¦å‘ç°ä¸€äº›ç«¯å€ªã€‚ä»£ç âœº21å¤„ï¼Œå°±æ˜¯å½“POOLç›‘å¬åœæ­¢çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¿™ä¸ªfutureä¼šå¾—åˆ°é€šçŸ¥ï¼Œç„¶åä¼šæ‰§è¡Œ_pollè¿™ä¸ªå›è°ƒã€‚ä»£ç âœº22å¤„ï¼Œå°±æ˜¯å¼€å§‹ç›‘å¬è¿™ä¸ªsocketæ˜¯å¦æœ‰æ•°æ®åˆ°æ¥ã€‚æ‰€ä»¥æˆ‘ä»¬å…ˆåˆ†æpolledå‡½æ•°ï¼Œä»è¿™ä¸ªå‡½æ•°åç§°ä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œå·²ç»æ˜¯æœ‰æ•°æ®åˆ°æ¥äº†ï¼Œæ‰€ä»¥æ‰åŠ äº†edè¿™ä¸ªåç¼€ï¼Œè¡¨ç¤ºäº‹æƒ…å·²ç»å‘ç”ŸğŸ˜›ã€‚
 
 å›è¿‡å¤´æ¥çœ‹polledå‡½æ•°ï¼Œå¾ˆæ˜æ˜¾ï¼Œreventså°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ç»“æœï¼Œè¿™æ—¶å€™æŠŠç›¸å…³çš„promiseè®¾ç½®è¿”å›å€¼ï¼Œå…³è”çš„futureå°±å¾—åˆ°é€šçŸ¥äº†ã€‚ä½†æ˜¯ä»–å¾—åˆ°çš„æ˜¯onReadyé€šçŸ¥ã€‚è¿™æ—¶å€™acceptå‡½æ•°å°±å¯ä»¥è¿”å›äº†ã€‚å‡½æ•°æˆåŠŸè¿”å›äº†ä¹‹åå°±ä¼šè°ƒç”¨thenè®¾ç½®çš„å›è°ƒå‡½æ•°ï¼Œå°±æ˜¯acceptå‡½æ•°ã€‚è¿™æ—¶å€™å¼€å§‹æ­£å¸¸çš„ç½‘ç»œæµç¨‹ï¼Œå°±æ˜¯è°ƒç”¨network::acceptè¿›è¡Œç³»ç»Ÿè°ƒç”¨ã€‚è®¾ç½®socketçš„ä¸€äº›åŸºæœ¬å±æ€§ï¼Œç„¶åè¿”å›Socketå¯¹è±¡ã€‚è¿™æ—¶å€™è¿”å›çš„Futureå®é™…ä¸Šè¦ä¹ˆæ˜¯å¤±è´¥çš„ï¼Œè¦ä¹ˆæ˜¯æˆåŠŸè®¾ç½®å€¼çš„ï¼Œå°±æ˜¯Futureå·²ç»æœ‰ç»“è®ºäº†ã€‚å› ä¸ºä»–æ˜¯ä»ä¸€ä¸ªTryå¯¹è±¡åˆå§‹åŒ–çš„ã€‚
 
 ä¸‹é¢ç®€å•çœ‹ä¸€ä¸‹Futureçš„è¿™ä¸ªæ„é€ å‡½æ•°ï¼š
 
~~~cpp
  
template <typename T>
Future<T>::Future(const Try<T>& t)
  : data(new Data())
{
  if (t.isSome()){
    set(t.get());
  } else {
    fail(t.error());
  }
}

~~~


è¯´åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ä»‹ç»Future<T>::thenå‡½æ•°ã€‚å› ä¸ºå‰é¢æˆ‘ä»¬ä¹Ÿçœ‹åˆ°äº†ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æŸä¸ªå¼‚æ­¥å‡½æ•°è°ƒç”¨æˆåŠŸä¹‹åå†å¯ç”¨æˆ‘ä»¬çš„acceptæœºåˆ¶ã€‚å°½ç®¡æ‰€æœ‰çš„æ“ä½œéƒ½æ˜¯å¼‚æ­¥çš„ï¼Œä½†æ˜¯æœ‰æ—¶å€™ä¸åŒçš„æ“ä½œæ—¶é—´è¿˜æ˜¯éœ€è¦åŒæ­¥å®Œæˆã€‚


~~~cpp
template <typename T>
template <typename X>
Future<X> Future<T>::then(const lambda::function<X(const T&)>& f) const
{
  std::shared_ptr<Promise<X>> promise(new Promise<X>());

  lambda::function<void(const Future<T>&)> then =
    lambda::bind(&internal::then<T, X>, f, promise, lambda::_1);  âœº30

  onAny(then);

  // Propagate discarding up the chain. To avoid cyclic dependencies,
  // we keep a weak future in the callback.
  promise->future().onDiscard(
      lambda::bind(&internal::discard<T>, WeakFuture<T>(*this))); âœº31

  return promise->future();
}
~~~
è¿™é‡Œä¸ºå•¥è¦æœ‰ä¸¤ä¸ªtemplateå‘¢ï¼Œå½“ç„¶æ˜¯ä¸ºäº†åé¢å‡½æ•°çš„è¿”å›å€¼äº†ã€‚ä»ä¸Šé¢å‡½æ•°å¯ä»¥çœ‹å‡ºï¼Œé¦–å…ˆnewäº†ä¸€ä¸ªæ–°çš„promiseï¼Œè¿™ä¸ªPromiseå½“ç„¶æ˜¯è¿”å›Xçš„å€¼äº†ã€‚âœº30æ˜¾ç¤ºå‰é¢çš„Futureçš„ä»»ä½•çš„çŠ¶æ€å˜åŒ–éƒ½æ˜¯è¦å›è°ƒthenè¿™ä¸ªå‡½æ•°çš„ï¼Œè¿™æ ·æˆ‘ä»¬å°±å®Œæˆäº†åŒæ­¥ï¼Œæ˜¯ä¸æ˜¯ï¼Ÿç¬¬ä¸€ä¸ªFutureçš„çŠ¶æ€å˜åŒ–è¡¨ç¤ºç¬¬ä¸€ä¸ªå·²ç»å®Œæˆï¼Œæ‰€ä»¥æ‰ä¼šè°ƒç”¨ç¬¬äºŒä¸ªï¼ï¼Œä½†æ˜¯ä¸‡ä¸€ç”¨æˆ·Canceläº†ç¬¬äºŒæ­¥çš„æ“ä½œå‘¢ï¼Ÿå‰é¢çš„Futureä¹Ÿå¿…é¡»å¾—åˆ°é€šçŸ¥æ˜¯ä¸æ˜¯ï¼Ÿè¿™å°±æ˜¯âœº31è¡Œä»£ç çš„ä½œç”¨ï¼Œåå‘åé¦ˆï¼æ•´æ¡é“¾æ¡å…¨éƒ¨å–æ¶ˆï¼

æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹internal::thenåšäº†ä»€ä¹ˆï¼Ÿ

~~~cpp
template <typename T, typename X>
void then(const lambda::function<X(const T&)>& f,
          const std::shared_ptr<Promise<X>>& promise,
          const Future<T>& future)
{
  if (future.isReady()) {
    if (future.hasDiscard()) {
      promise->discard();
    } else {
      promise->set(f(future.get()));
    }
  } else if (future.isFailed()) {
    promise->fail(future.failure());
  } else if (future.isDiscarded()) {
    promise->discard();
  }
}
~~~

å¦‚æœFutureçŠ¶æ€ä¸ºREADYçš„è¯ï¼Œå°±è°ƒç”¨getï¼Œç„¶åè°ƒç”¨fè¿™ä¸ªå‡½æ•°ï¼Œè¿™æ ·è°ƒç”¨é“¾å°±èµ·æ¥äº†ã€‚åŒæ—¶ä¹Ÿè€ƒè™‘äº†å…¶ä»–çš„çŠ¶æ€æƒ…å†µã€‚ä»ä¸Šé¢æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œä¸€ä¸ªFutureï¼Œåœ¨ä»»ä½•çŠ¶æ€ä¸‹éƒ½æœ‰å¯èƒ½è¢«discardã€‚è¿™é‡Œå¼•ç”³ä¸€ä¸‹Promiseçš„discardå’ŒFutureçš„discardçš„ä¸åŒï¼Œå½“ä¸¢å¼ƒPromiseçš„æ—¶å€™ï¼ŒFutureçš„çŠ¶æ€æ‰æ˜¯DISCARDEDçŠ¶æ€ï¼Œå½“ä¸¢å¼ƒFutureçš„æ—¶å€™ï¼Œä¸ä¼šä¿®æ”¹çŠ¶æ€ï¼Œåªæ˜¯æ ‡è®°discardï¼Œå¯ä»¥ç†è§£ä¸ºï¼šç®¡é“çš„å†™ç«¯ä¸å…³é—­ï¼Œè¯»ç«¯åšä¸ªæ ‡è®°ï¼Œè¡¨ç¤ºæ˜¯å¦å¯è¯»ã€‚å†™ç«¯ä¸€æ—¦å…³é—­ï¼Œè¯»ç«¯ç«‹å³å…³é—­ã€‚æ³¨æ„50å’Œ51å¤„çš„åŒºåˆ«ã€‚

~~~cpp
template <typename T>
bool Promise<T>::discard(Future<T> future)
{
  std::shared_ptr<typename Future<T>::Data> data = future.data;

  bool result = false;

  synchronized (data->lock) {
    if (data->state == Future<T>::PENDING) {  âœº50
      data->state = Future<T>::DISCARDED;
      result = true;
    }
  }
  ...
  return result;
}

template <typename T>
bool Future<T>::discard()
{
  bool result = false;

  std::vector<DiscardCallback> callbacks;
  synchronized (data->lock) {
    if (!data->discard && data->state == PENDING) {
      result = data->discard = true;      âœº51

       callbacks = data->onDiscardCallbacks;
      data->onDiscardCallbacks.clear();
    }
  }
  ...

  return result;
}

~~~

OKï¼Œæˆ‘ä»¬ç»§ç»­åˆ†æprocess::initializeå¤„çš„ç¬¬3å¤„ä»£ç ï¼š

~~~cpp
__s__->accept()
    .onAny(lambda::bind(&internal::on_accept, lambda::_1));
~~~

è¿™æ—¶å€™ç½‘ç»œIOèµ°åˆ°äº†acceptè¿™ä¸€æ­¥ï¼ŒçœŸçš„ä¸å®¹æ˜“ï¼

~~~cpp
//[process.cpp]

void on_accept(const Future<Socket>& socket)
{
  if (socket.isReady()) {
    // Inform the socket manager for proper bookkeeping.
    socket_manager->accepted(socket.get());

    const size_t size = 80 * 1024;
    char* data = new char[size];
    memset(data, 0, size);

    DataDecoder* decoder = new DataDecoder(socket.get());

    socket.get().recv(data, size)
      .onAny(lambda::bind(
          &internal::decode_recv,
          lambda::_1,
          data,
          size,
          new Socket(socket.get()),
          decoder));
  }

  __s__->accept()
    .onAny(lambda::bind(&on_accept, lambda::_1));
}
~~~

è¿™ä¸€æ­¥ï¼ŒSocketå·²ç»å‡†å¤‡å¥½äº†ï¼Œç­‰å¾…è¿™ä¸ªSocketçš„IOåŠ¨ä½œï¼Œå¹¶ä¸”ç»§ç»­ç­‰å¾…ä¸‹ä¸€ä¸ªè¿æ¥åˆ°æ¥ã€‚
SocketManagerä¿å­˜è¿™ä¸ªSocketä»¥åšåç”¨ã€‚

~~~cpp
void SocketManager::accepted(const Socket& socket)
{
  synchronized (mutex) {
    sockets[socket] = new Socket(socket);
  }
}

//[poll_socket.cpp]

Future<size_t> PollSocketImpl::recv(char* data, size_t size)
{
  return io::read(get(), data, size);
}

~~~

~~~cpp
//[io.cpp]
const size_t BUFFERED_READ_SIZE = 16*4096;

Future<size_t> read(int fd, void* data, size_t size)
{
  process::initialize();

  std::shared_ptr<Promise<size_t>> promise(new Promise<size_t>());

  // Check the file descriptor.
  Try<bool> nonblock = os::isNonblock(fd);
  if (nonblock.isError()) {
    // The file descriptor is not valid (e.g., has been closed).
    promise->fail(
        "Failed to check if file descriptor was non-blocking: " +
        nonblock.error());
    return promise->future();
  } else if (!nonblock.get()) {
    // The file descriptor is not non-blocking.
    promise->fail("Expected a non-blocking file descriptor");
    return promise->future();
  }

  // Because the file descriptor is non-blocking, we call read()
  // immediately. The read may in turn call poll if necessary,
  // avoiding unnecessary polling. We also observed that for some
  // combination of libev and Linux kernel versions, the poll would
  // block for non-deterministically long periods of time. This may be
  // fixed in a newer version of libev (we use 3.8 at the time of
  // writing this comment).
  internal::read(fd, data, size, internal::NONE, promise, io::READ);

  return promise->future();
}
~~~

~~~cpp

void read(
    int fd,
    void* data,
    size_t size,
    ReadFlags flags,
    const std::shared_ptr<Promise<size_t>>& promise,
    const Future<short>& future)
{
  // Ignore this function if the read operation has been discarded.
  if (promise->future().hasDiscard()) {
    CHECK(!future.isPending());
    promise->discard();
    return;
  }

  if (size == 0) {
    promise->set(0);
    return;
  }

  if (future.isDiscarded()) {
    promise->fail("Failed to poll: discarded future");
  } else if (future.isFailed()) {
    promise->fail(future.failure());
  } else {
    ssize_t length;
    if (flags == NONE) {
      length = ::read(fd, data, size);
    } else { // PEEK.
      // In case 'fd' is not a socket ::recv() will fail with ENOTSOCK and the
      // error will be propagted out.
      length = ::recv(fd, data, size, MSG_PEEK);
    }

    if (length < 0) {
      if (errno == EINTR || errno == EAGAIN || errno == EWOULDBLOCK) {
        // Restart the read operation.
        Future<short> future =
          io::poll(fd, process::io::READ).onAny(
              lambda::bind(&internal::read,
                           fd,
                           data,
                           size,
                           flags,
                           promise,
                           lambda::_1));

        // Stop polling if a discard occurs on our future.
        promise->future().onDiscard(
            lambda::bind(&process::internal::discard<short>,
                         WeakFuture<short>(future)));
      } else {
        // Error occurred.
        promise->fail(os::strerror(errno));
      }
    } else {
      promise->set(length);
    }
  }
}
~~~

ä»è¿™é‡Œçœ‹å‡ºï¼Œå¼‚æ­¥å°±æ˜¯ä¸€ç§æœ‰åºçš„å¾ªç¯ã€‚æ³¨æ„è¿™é‡Œæ˜¯æ€ä¹ˆå¤„ç†EINTR, EAGAIN ç­‰å¸¸è§ç½‘ç»œé”™è¯¯çš„ã€‚

è‡³æ­¤ï¼Œæ•°æ®å·²ç»è¯»å–å®Œæ¯•ï¼Œå¯ä»¥å¼€å§‹è§£ææ•°æ®äº†ã€‚ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œrecvè·å–äº†æ•°æ®ä¹‹åä¼šå›è°ƒdecode_recvå‡½æ•°ã€‚

~~~cpp
void decode_recv(
    const Future<size_t>& length,
    char* data,
    size_t size,
    Socket* socket,
    DataDecoder* decoder)
{
  ...
  // Decode as much of the data as possible into HTTP requests.
  const deque<Request*> requests = decoder->decode(data, length.get());

  ...

  if (!requests.empty()) {
    // Get the peer address to augment the requests.
    Try<Address> address = socket->peer();

	...

    foreach (Request* request, requests) {
      request->client = address.get();
      process_manager->handle(decoder->socket(), request);
    }
  }

  socket->recv(data, size)
    .onAny(lambda::bind(&decode_recv, lambda::_1, data, size, socket, decoder));
}
~~~
è§£æå®Œäº†requestï¼Œç»§ç»­æ¥å—æ•°æ®ï¼Œå¹¶ä¸”è§£æè¿™ä¸€æµç¨‹

è¾…åŠ©å‡½æ•°ï¼Œç½—åˆ—ä¸€ä¸‹ï¼š

~~~cpp
static Message* parse(Request* request)
{
  // TODO(benh): Do better error handling (to deal with a malformed
  // libprocess message, malicious or otherwise).

  // First try and determine 'from'.
  Option<UPID> from = None();

  if (request->headers.contains("Libprocess-From")) {
    from = UPID(strings::trim(request->headers["Libprocess-From"]));
  } else {
    // Try and get 'from' from the User-Agent.
    const string& agent = request->headers["User-Agent"];
    const string identifier = "libprocess/";
    size_t index = agent.find(identifier);
    if (index != string::npos) {
      from = UPID(agent.substr(index + identifier.size(), agent.size()));
    }
  }

  if (from.isNone()) {
    return NULL;
  }

  // Now determine 'to'.
  size_t index = request->url.path.find('/', 1);
  index = index != string::npos ? index - 1 : string::npos;

  // Decode possible percent-encoded 'to'.
  Try<string> decode = http::decode(request->url.path.substr(1, index));

  if (decode.isError()) {
    VLOG(2) << "Failed to decode URL path: " << decode.get();
    return NULL;
  }

  //!!!è¿™é‡Œçš„decodeå¾ˆé‡è¦ï¼Œæ˜¯UPIDçš„idï¼Œè¿™é‡Œå¯¹åº”çš„æ˜¯æŸä¸ªprocessï¼Œè§£æå‡ºæ¥çš„äº‹ä»¶ä¼šæ”¾åˆ°è¿™ä¸ªprocessçš„äº‹ä»¶é˜Ÿåˆ—é‡Œé¢å»
  const UPID to(decode.get(), __address__);

  // And now determine 'name'.
  index = index != string::npos ? index + 2: request->url.path.size();
  const string name = request->url.path.substr(index);

  VLOG(2) << "Parsed message name '" << name
          << "' for " << to << " from " << from.get();

  Message* message = new Message();
  message->name = name;
  message->from = from.get();
  message->to = to;
  message->body = request->body;

  return message;
}

ProcessReference ProcessManager::use(const UPID& pid)
{
  if (pid.address == __address__) {
    synchronized (processes_mutex) {
      if (processes.count(pid.id) > 0) {
        // Note that the ProcessReference constructor _must_ get
        // called while holding the lock on processes so that waiting
        // for references is atomic (i.e., race free).
        return ProcessReference(processes[pid.id]);
      }
    }
  }

  return ProcessReference(NULL);
}

class ProcessManager
{
map<string, ProcessBase*> processes;
}

bool ProcessManager::deliver(
    const UPID& to,
    Event* event,
    ProcessBase* sender /* = NULL*/ )
{
  CHECK(event != NULL);

  if (ProcessReference receiver = use(to)) {
    return deliver(receiver, event, sender);
  }
  VLOG(2) << "Dropping event for process " << to;

  delete event;
  return false;
}

bool ProcessManager::deliver(
    ProcessBase* receiver,
    Event* event,
    ProcessBase* sender /* = NULL */)
{
  CHECK(event != NULL);

  ...

  receiver->enqueue(event);

  return true;
}

void ProcessBase::enqueue(Event* event, bool inject)
{
  CHECK(event != NULL);

  synchronized (mutex) {
    if (state != TERMINATING && state != TERMINATED) {
      if (!inject) {
        events.push_back(event);
      } else {
        events.push_front(event);
      }

      if (state == BLOCKED) {
        state = READY;
        process_manager->enqueue(this);
      }

      CHECK(state == BOTTOM ||
            state == READY ||
            state == RUNNING);
    } else {
      delete event;
    }
  }
}

ProcessBase* ProcessManager::dequeue()
{
  ProcessBase* process = NULL;

  synchronized (runq_mutex) {
    if (!runq.empty()) {
      process = runq.front();
      runq.pop_front();
      // Increment the running count of processes in order to support
      // the Clock::settle() operation (this must be done atomically
      // with removing the process from the runq).
      running.fetch_add(1);
    }
  }

  return process;
}

~~~

è¿™æ—¶å€™å°±æŠŠæ—¶é—´æ”¾åˆ°äº†æœ¬æœºçš„æŸä¸ªProcessçš„eventé˜Ÿåˆ—é‡Œé¢å»äº†

~~~cpp

//[process.cpp:2256]
void ProcessManager::handle(
    const Socket& socket,
    Request* request)
{
  CHECK(request != NULL);

  // Check if this is a libprocess request (i.e., 'User-Agent:
  // libprocess/id@ip:port') and if so, parse as a message.
  if (libprocess(request)) {
    Message* message = parse(request);
    if (message != NULL) {
      // TODO(benh): Use the sender PID when delivering in order to
      // capture happens-before timing relationships for testing.
      bool accepted = deliver(message->to, new MessageEvent(message));

      // Get the HttpProxy pid for this socket.
      PID<HttpProxy> proxy = socket_manager->proxy(socket);

      // Only send back an HTTP response if this isn't from libprocess
      // (which we determine by looking at the User-Agent). This is
      // necessary because older versions of libprocess would try and
      // recv the data and parse it as an HTTP request which would
      // fail thus causing the socket to get closed (but now
      // libprocess will ignore responses, see ignore_data).
      Option<string> agent = request->headers.get("User-Agent");
      if (agent.getOrElse("").find("libprocess/") == string::npos) {
        if (accepted) {
          VLOG(2) << "Accepted libprocess message to " << request->url.path;
          dispatch(proxy, &HttpProxy::enqueue, Accepted(), *request);
        } else {
          VLOG(1) << "Failed to handle libprocess message to "
                  << request->url.path << ": not found";
          dispatch(proxy, &HttpProxy::enqueue, NotFound(), *request);
        }
      }

      delete request;
      return;
    }

    VLOG(1) << "Failed to handle libprocess message: "
            << request->method << " " << request->url.path
            << " (User-Agent: " << request->headers["User-Agent"] << ")";

    delete request;
    return;
  }

  // Treat this as an HTTP request. Start by checking that the path
  // starts with a '/' (since the code below assumes as much).
  if (request->url.path.find('/') != 0) {
    VLOG(1) << "Returning '400 Bad Request' for '" << request->url.path << "'";

    // Get the HttpProxy pid for this socket.
    PID<HttpProxy> proxy = socket_manager->proxy(socket);

    // Enqueue the response with the HttpProxy so that it respects the
    // order of requests to account for HTTP/1.1 pipelining.
    dispatch(proxy, &HttpProxy::enqueue, BadRequest(), *request);

    // Cleanup request.
    delete request;
    return;
  }

  // Ignore requests with relative paths (i.e., contain "/..").
  if (request->url.path.find("/..") != string::npos) {
    VLOG(1) << "Returning '404 Not Found' for '" << request->url.path
            << "' (ignoring requests with relative paths)";

    // Get the HttpProxy pid for this socket.
    PID<HttpProxy> proxy = socket_manager->proxy(socket);

    // Enqueue the response with the HttpProxy so that it respects the
    // order of requests to account for HTTP/1.1 pipelining.
    dispatch(proxy, &HttpProxy::enqueue, NotFound(), *request);

    // Cleanup request.
    delete request;
    return;
  }

  // Split the path by '/'.
  vector<string> tokens = strings::tokenize(request->url.path, "/");

  // Try and determine a receiver, otherwise try and delegate.
  UPID receiver;

  if (tokens.size() == 0 && delegate != "") {
    request->url.path = "/" + delegate;
    receiver = UPID(delegate, __address__);
  } else if (tokens.size() > 0) {
    // Decode possible percent-encoded path.
    Try<string> decode = http::decode(tokens[0]);
    if (!decode.isError()) {
      receiver = UPID(decode.get(), __address__);
    } else {
      VLOG(1) << "Failed to decode URL path: " << decode.error();
    }
  }

  if (!use(receiver) && delegate != "") {
    // Try and delegate the request.
    request->url.path = "/" + delegate + request->url.path;
    receiver = UPID(delegate, __address__);
  }

  synchronized (firewall_mutex) {
    // Don't use a const reference, since it cannot be guaranteed
    // that the rules don't keep an internal state.
    foreach (Owned<firewall::FirewallRule>& rule, firewallRules) {
      Option<Response> rejection = rule->apply(socket, *request);
      if (rejection.isSome()) {
        VLOG(1) << "Returning '"<< rejection.get().status << "' for '"
                << request->url.path << "' (firewall rule forbids request)";

        // TODO(arojas): Get rid of the duplicated code to return an
        // error.

        // Get the HttpProxy pid for this socket.
        PID<HttpProxy> proxy = socket_manager->proxy(socket);

        // Enqueue the response with the HttpProxy so that it respects
        // the order of requests to account for HTTP/1.1 pipelining.
        dispatch(
            proxy,
            &HttpProxy::enqueue,
            rejection.get(),
            *request);

        // Cleanup request.
        delete request;
        return;
      }
    }
  }

  if (use(receiver)) {
    // The promise is created here but its ownership is passed
    // into the HttpEvent created below.
    Promise<Response>* promise(new Promise<Response>());

    PID<HttpProxy> proxy = socket_manager->proxy(socket);

    // Enqueue the response with the HttpProxy so that it respects the
    // order of requests to account for HTTP/1.1 pipelining.
    dispatch(proxy, &HttpProxy::handle, promise->future(), *request);

    // TODO(benh): Use the sender PID in order to capture
    // happens-before timing relationships for testing.
    deliver(receiver, new HttpEvent(request, promise));

    return;
  }

  // This has no receiver, send error response.
  VLOG(1) << "Returning '404 Not Found' for '" << request->url.path << "'";

  // Get the HttpProxy pid for this socket.
  PID<HttpProxy> proxy = socket_manager->proxy(socket);

  // Enqueue the response with the HttpProxy so that it respects the
  // order of requests to account for HTTP/1.1 pipelining.
  dispatch(proxy, &HttpProxy::enqueue, NotFound(), *request);

  // Cleanup request.
  delete request;
}
~~~

ä¸Šè¿°ä¸»è¦æ˜¯è§£æhttpæ•°æ®ï¼Œç”ŸæˆEventï¼Œç„¶ååŠ å…¥åˆ°æœ¬æœºçš„ProcessBaseå¯¹åº”çš„äº‹ä»¶é˜Ÿåˆ—é‡Œé¢å»ã€‚

æ³¨æ„processesè¿™ä¸ªæˆå‘˜å˜é‡ï¼Œæ¯æ¬¡spawnçš„æ—¶å€™éƒ½ä¼šåŠ å…¥processåˆ°è¿™ä¸ªå˜é‡ä¸­ï¼Œè¿™æ ·è¿™ä¸ªprocesså°±å¯ä»¥å¤„ç†äº‹ä»¶äº†ã€‚

ç»§ç»­æœ€å‰é¢ï¼Œçº¿ç¨‹ä»é˜Ÿåˆ—é‡Œé¢å–å‡ºæ•°æ®åï¼Œä¼šè°ƒç”¨process_manager->resume(process);
è¿™ä¸ªå‡½æ•°è´Ÿè´£è°ƒç”¨processçš„

~~~cpp

void ProcessManager::resume(ProcessBase* process)
{
  __process__ = process;

  VLOG(2) << "Resuming " << process->pid << " at " << Clock::now();

  bool terminate = false;
  bool blocked = false;

  CHECK(process->state == ProcessBase::BOTTOM ||
        process->state == ProcessBase::READY);

  if (process->state == ProcessBase::BOTTOM) {
    process->state = ProcessBase::RUNNING;
    try { process->initialize(); }
    catch (...) { terminate = true; }
  }

  while (!terminate && !blocked) {
    Event* event = NULL;

    synchronized (process->mutex) {
      if (process->events.size() > 0) {
        event = process->events.front();
        process->events.pop_front();
        process->state = ProcessBase::RUNNING;
      } else {
        process->state = ProcessBase::BLOCKED;
        blocked = true;
      }
    }

    if (!blocked) {
      CHECK(event != NULL);

      // Determine if we should filter this event.
      synchronized (filterer_mutex) {
        if (filterer != NULL) {
          bool filter = false;
          struct FilterVisitor : EventVisitor
          {
            explicit FilterVisitor(bool* _filter) : filter(_filter) {}

            virtual void visit(const MessageEvent& event)
            {
              *filter = filterer->filter(event);
            }

            virtual void visit(const DispatchEvent& event)
            {
              *filter = filterer->filter(event);
            }

            virtual void visit(const HttpEvent& event)
            {
              *filter = filterer->filter(event);
            }

            virtual void visit(const ExitedEvent& event)
            {
              *filter = filterer->filter(event);
            }

            bool* filter;
          } visitor(&filter);

          event->visit(&visitor);

          if (filter) {
            delete event;
            continue; // Try and execute the next event.
          }
        }
      }

      // Determine if we should terminate.
      terminate = event->is<TerminateEvent>();

      // Now service the event.
      try {
        process->serve(*event);
      } catch (const std::exception& e) {
        std::cerr << "libprocess: " << process->pid
                  << " terminating due to "
                  << e.what() << std::endl;
        terminate = true;
      } catch (...) {
        std::cerr << "libprocess: " << process->pid
                  << " terminating due to unknown exception" << std::endl;
        terminate = true;
      }

      delete event;

      if (terminate) {
        cleanup(process);
      }
    }
  }

  __process__ = NULL;

  CHECK_GE(running.load(), 1);
  running.fetch_sub(1);
}
~~~


resumeè¿™é‡Œæ˜¯æœ€ä¸»è¦çš„ï¼Œä»–ä¼šå®ç°å¯¹Processçš„åˆå§‹åŒ–é€»è¾‘ï¼Œè°ƒç”¨ä»–çš„äº‹ä»¶å¤„ç†æœºåˆ¶ã€‚è¿™é‡Œçš„äº‹ä»¶å¤„ç†éƒ½æ˜¯åœ¨å•ä¸ªçº¿ç¨‹é‡Œé¢å®Œæˆçš„ã€‚å¯ä»¥å®ç°ç±»ä¼¼ç¡®ä¿æŸä¸ªé€»è¾‘çº¿ç¨‹å®‰å…¨çš„æ€è·¯ã€‚è‡³æ­¤Processçš„è¿è¡Œæœºåˆ¶å¤§æ¦‚æœ‰äº†äº†è§£ï¼Œç„¶åæˆ‘ä»¬çœ‹çœ‹è¿›ç¨‹Processæ˜¯æ€ä¹ˆåŠ å…¥åˆ°ç³»ç»Ÿçš„ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬ç»§ç»­åˆ†æè¿™ä¸€è¡Œä»£ç ï¼š

~~~cpp
gc = spawn(new GarbageCollector());//4ï¸âƒ£

UPID spawn(ProcessBase* process, bool manage)
{
  process::initialize();

  if (process != NULL) {
    // If using a manual clock, try and set current time of process
    // using happens before relationship between spawner (__process__)
    // and spawnee (process)!
    if (Clock::paused()) {
      Clock::update(process, Clock::now(__process__));
    }

    return process_manager->spawn(process, manage);
  } else {
    return UPID();
  }
}

UPID ProcessManager::spawn(ProcessBase* process, bool manage)
{
  CHECK(process != NULL);

  synchronized (processes_mutex) {
    if (processes.count(process->pid.id) > 0) {
      return UPID();
    } else {
      processes[process->pid.id] = process;
    }
  }

  // Use the garbage collector if requested.
  if (manage) {
    dispatch(gc, &GarbageCollector::manage<ProcessBase>, process);
  }

   UPID pid = process->self();

  enqueue(process);
  
  return pid;
}

void ProcessManager::enqueue(ProcessBase* process)
{
...

  synchronized (runq_mutex) {
    CHECK(find(runq.begin(), runq.end(), process) == runq.end());
    runq.push_back(process);
  }

  // Wake up the processing thread if necessary.
  gate->open();
}

~~~

spawnä¸»è¦æ˜¯æŠŠè¿™ä¸ªPRocess enqueueåˆ°è‡ªå·±çš„processé˜Ÿåˆ—ä¸­ï¼Œè¿™æ ·å°±å¯ä»¥ç»™è¿™ä¸ªprocesså‘é€äº‹ä»¶ï¼Œæ¥è®©ProcessManager resumeä»–å¤„ç†äº‹ä»¶äº†~

æœ€åçœ‹çœ‹terminate

~~~cpp
void ProcessManager::terminate(
    const UPID& pid,
    bool inject,
    ProcessBase* sender)
{
  if (ProcessReference process = use(pid)) {
    if (Clock::paused()) {
      Clock::update(process, Clock::now(sender != NULL ? sender : __process__));
    }

    if (sender != NULL) {
      process->enqueue(new TerminateEvent(sender->self()), inject);
    } else {
      process->enqueue(new TerminateEvent(UPID()), inject);
    }
  }
}

void dispatch(
    const UPID& pid,
    const std::shared_ptr<lambda::function<void(ProcessBase*)>>& f,
    const Option<const std::type_info*>& functionType)
{
  process::initialize();

  DispatchEvent* event = new DispatchEvent(pid, f, functionType);
  process_manager->deliver(pid, event, __process__);
}
~~~


æ³¨æ„ï¼šProcessçš„ç”Ÿå‘½å‘¨æœŸï¼šåˆ›å»ºæ˜¯å¤–é¢åˆ›å»ºçš„ï¼ŒProcessMangerçš„æˆå‘˜å˜é‡processesä¿å­˜äº†æ‰€æœ‰çš„Processå®ä¾‹ï¼Œä¸€èˆ¬å¦‚æœä¸æ˜¯ç³»ç»ŸGCè‡ªåŠ¨ç®¡ç†çš„è¯ï¼Œæ˜¯ä¸ä¼šé”€æ¯çš„ã€‚å‰é¢çš„spawnå’Œresumeåªæ˜¯ä¸æ–­çš„æŠŠè¿™äº›å®ä¾‹å…¥é˜Ÿå’Œå‡ºé˜Ÿåˆ—å¤„ç†æ•°æ®è€Œå·²ã€‚


è‡³æ­¤ï¼Œæµç¨‹å·²ç»åˆ†ææ¸…æ¥šäº†ï¼

ProcessBaseçš„å‡ ä¸ªå‡½æ•°å¯¹äºæˆ‘ä»¬äº†è§£ä¹Ÿå¾ˆé‡è¦ï¼Œç½—åˆ—å¦‚ä¸‹ï¼š
éƒ½å¾ˆç®€å•ï¼Œå¤æ‚çš„åé¢ä¼šè§£é‡Šä¸€ä¸‹ã€‚

~~~cpp
ProcessBase::ProcessBase(const string& id)
{
  process::initialize();

  state = ProcessBase::BOTTOM;

  refs = 0;

  pid.id = id != "" ? id : ID::generate();
  pid.address = __address__;

  // If using a manual clock, try and set current time of process
  // using happens before relationship between creator (__process__)
  // and createe (this)!
  if (Clock::paused()) {
    Clock::update(this, Clock::now(__process__), Clock::FORCE);
  }
}


ProcessBase::~ProcessBase() {}


void ProcessBase::enqueue(Event* event, bool inject)
{
  CHECK(event != NULL);

  synchronized (mutex) {
    if (state != TERMINATING && state != TERMINATED) {
      if (!inject) {
        events.push_back(event);
      } else {
        events.push_front(event);
      }

      if (state == BLOCKED) {
        state = READY;
        process_manager->enqueue(this);
      }

      CHECK(state == BOTTOM ||
            state == READY ||
            state == RUNNING);
    } else {
      delete event;
    }
  }
}


void ProcessBase::inject(
    const UPID& from,
    const string& name,
    const char* data,
    size_t length)
{
  if (!from)
    return;

  Message* message = encode(from, pid, name, string(data, length));

  enqueue(new MessageEvent(message), true);
}


void ProcessBase::send(
    const UPID& to,
    const string& name,
    const char* data,
    size_t length)
{
  if (!to) {
    return;
  }

  // Encode and transport outgoing message.
  transport(encode(pid, to, name, string(data, length)), this);
}


void ProcessBase::visit(const MessageEvent& event)
{
  if (handlers.message.count(event.message->name) > 0) {
    handlers.message[event.message->name](
        event.message->from,
        event.message->body);
  } else if (delegates.count(event.message->name) > 0) {
    VLOG(1) << "Delegating message '" << event.message->name
            << "' to " << delegates[event.message->name];
    Message* message = new Message(*event.message);
    message->to = delegates[event.message->name];
    transport(message, this);
  }
}


void ProcessBase::visit(const DispatchEvent& event)
{
  (*event.f)(this);
}


//å‚è€ƒä¸€ä¸‹dispatchçš„å®ç°ï¼š
dispatchçš„åŸå‹æ˜¯ï¼švoid(ProcessBase*)ï¼›
æ‰€ä»¥ä¸Šé¢çš„é‚£ä¸€è¡Œ(*event.f)(this);çš„thiså°±å®¹æ˜“ç†è§£äº†ã€‚

struct DispatchEvent : Event
{
  DispatchEvent(
      const UPID& _pid,
      const std::shared_ptr<lambda::function<void(ProcessBase*)>>& _f,
      const Option<const std::type_info*>& _functionType)
    : pid(_pid),
      f(_f),
      functionType(_functionType)
  {}

  virtual void visit(EventVisitor* visitor) const
  {
    visitor->visit(*this);
  }

  // PID receiving the dispatch.
  const UPID pid;

  // Function to get invoked as a result of this dispatch event.
  const std::shared_ptr<lambda::function<void(ProcessBase*)>> f;

  const Option<const std::type_info*> functionType;

private:
  // Not copyable, not assignable.
  DispatchEvent(const DispatchEvent&);
  DispatchEvent& operator=(const DispatchEvent&);
};


void dispatch(
    const UPID& pid,
    const std::shared_ptr<lambda::function<void(ProcessBase*)>>& f,
    const Option<const std::type_info*>& functionType)
{
  process::initialize();

  DispatchEvent* event = new DispatchEvent(pid, f, functionType);
  process_manager->deliver(pid, event, __process__);
}

ä¸‹é¢ä¸¤ä¸ªå‡½æ•°existedå’Œfinalizeå¤„ç†ExitedEventå’ŒTerminateEvent

void ProcessBase::visit(const ExitedEvent& event)
{
  exited(event.pid);
}

void ProcessBase::visit(const TerminateEvent& event)
{
  finalize();
}

æœ‰å¤ªå¤šçš„routeï¼Œè¿™äº›å…¶å®éƒ½æ˜¯å¸®åŠ©ğŸ˜‚ï¼Œå“æˆ‘ä¸€è·³ã€‚
void ProcessBase::route(
    const string& name,
    const Option<string>& help_,
    const HttpRequestHandler& handler)
{
  // Routes must start with '/'.
  CHECK(name.find('/') == 0);

  HttpEndpoint endpoint;
  endpoint.handler = handler;

  handlers.http[name.substr(1)] = endpoint;

  dispatch(help, &Help::add, pid.id, name, help_);
}
~~~


##asyncå‡½æ•°

~~~cpp
template <typename F>
Future<Nothing> async(
    const F& f,
    typename boost::enable_if<boost::is_void<typename result_of<F()>::type> >::type*) // NOLINT(whitespace/line_length)
{
  return AsyncExecutor().execute(f);
}

class AsyncExecutor
{
  AsyncExecutor()
  {
    process = new AsyncExecutorProcess();
    spawn(process, true); // Automatically GC.
  }

  virtual ~AsyncExecutor() {}
}

  template <typename F>
  Future<typename result_of<F()>::type> execute(
      const F& f,
      typename boost::disable_if<boost::is_void<typename result_of<F()>::type> >::type* = NULL) // NOLINT(whitespace/line_length)
  {
    // Need to disambiguate overloaded method.
    typename result_of<F()>::type(AsyncExecutorProcess::*method)(const F&, typename boost::disable_if<boost::is_void<typename result_of<F()>::type> >::type*) = // NOLINT(whitespace/line_length)
      &AsyncExecutorProcess::execute<F>;

    return dispatch(process, method, f, (void*) NULL);
  }

~~~

å¾ˆæ˜æ˜¾ï¼Œè¿™é‡Œå°±æ˜¯newäº†ä¸€ä¸ªProcessï¼Œç„¶ådispatchäº†ä¸€ä¸ªæ–¹æ³•ã€‚å°±æŠŠæŸä¸ªå‡½æ•°å˜æˆå¼‚æ­¥çš„äº†ã€‚
